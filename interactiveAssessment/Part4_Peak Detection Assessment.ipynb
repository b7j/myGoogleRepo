{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "#### Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "#interactive web map package\n",
    "import folium \n",
    "from folium import features\n",
    "import shapefile\n",
    "from json import dumps\n",
    "from folium import plugins\n",
    "\n",
    "#Miscellanous\n",
    "from IPython.core.display import display #display inline package\n",
    "from ipywidgets import interact, interactive, fixed, FloatProgress\n",
    "import ipywidgets as widgets\n",
    "import pdb #debugging tool\n",
    "import pandas as pd # pandas dataframe package\n",
    "import pandas\n",
    "from datetime import datetime #date conversion tool\n",
    "from xlrd.xldate import xldate_as_tuple #xldate converter\n",
    "from urllib2 import urlopen #get data from web tool\n",
    "#from collections import OrderedDict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.display import Image\n",
    "import utm #lat long to utms\n",
    "import zipfile #unzip tool\n",
    "from IPython.display import clear_output\n",
    "from scipy.signal import correlate #rainfall correlation\n",
    "\n",
    "#numpy packages and tools\n",
    "import numpy as np\n",
    "from numpy import linspace\n",
    "from numpy.fft import fft, ifft, fft2, ifft2, fftshift\n",
    "\n",
    "#Spatial data packages\n",
    "import rasterio #io raster data \n",
    "#from osgeo import gdal\n",
    "\n",
    "#Plotly tools\n",
    "import plotly.tools as tls\n",
    "tls.set_credentials_file(username='NTPlotly', api_key='48kd2al3f2') #my plotly credentials (please dont use they cost me $$)\n",
    "import plotly.plotly as py\n",
    "#from plotly.graph_objs import *\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "import cufflinks as cf\n",
    "import matplotlib\n",
    "\n",
    "#scipy packages\n",
    "from scipy.misc import derivative #derivates of functions tool\n",
    "from scipy.interpolate import UnivariateSpline #basic spline fitting tool\n",
    "from scipy.signal import gaussian #gaussian filtering tool\n",
    "from scipy.ndimage import filters #anotehr filtering tool\n",
    "from scipy import stats #core scipy stats package\n",
    "from scipy.integrate import simps #simpsons rule package for trapezoid calcualtions in area under curve calcs\n",
    "\n",
    "#Basic math tools\n",
    "from math import log\n",
    "from math import factorial\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "#### NPV Peak Detection Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 4,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "An assessment of the npv peak detection method to identify major growth peaks using a classification of the Standard Precipitation Index. The following catergories have been used to assess the proportions of peaks in each class as a measure of the accuracy of the peak detection method to identify peaks associated with significat rainfall periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 9,
        "hidden": false,
        "row": 8,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "|SPI Start | SPI End | Catergory|\n",
    "|:--------:|:-------:|:---------:|\n",
    "| 2        |+2       |Extremely wet|\n",
    "|1.5       |1.99     |Very wet|\n",
    "|1.0       |1.49     |Moderately wet|\n",
    "|-0.99     |0.99     |Near normal|\n",
    "|-1.0      |-1.49    |Moderately dry|\n",
    "|-1.5      |-1.99    |Severely dry|\n",
    "|-2        |<-2      | Extremely dry |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 17,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e56b3e3183b40ccaa564825dc77c949"
      }
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Enter siteId from map exactly as it is displayed ie. 'hmd20' or 'bsp14a'\n",
    "text = widgets.Text(description=\"Site Id from map\")\n",
    "display(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 17,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "#### Standardised Precipitation Index Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#Function to download a time series of each field sites Standardised Precipitation Index (SPI) values. SPI has been calculated for a 20 month window for each 'standard' season for a time series between 1981 - 2014.\n",
    "def getSpiData(site,period):\n",
    "            \n",
    "    url = \"https://dl.dropboxusercontent.com/u/53285700/csvs/spiOutput\" + period + \".csv\" #adds the chosen period to the url NOTE: 36 months appears to be the most representative of an arid climate\n",
    "    \n",
    "    u = urlopen(url)\n",
    "    data = u.read()\n",
    "    u.close()\n",
    "    filename = \"data/spi.csv\"\n",
    " \n",
    "    with open(filename, \"wb\") as f :\n",
    "        f.write(data)\n",
    "        \n",
    "    df = pandas.read_csv(filename,header =0,parse_dates=['season'],dayfirst=True)\n",
    "    \n",
    "    mask = df['siteId']==site\n",
    "    \n",
    "    newDf = df[mask]\n",
    "    \n",
    "    return newDf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def getOutputData(filename,pickleName,x,y):\n",
    "    \n",
    "    df = pd.read_csv(filename,header=0)\n",
    "    \n",
    "    df2 = pd.read_pickle(pickleName)\n",
    "    \n",
    "    #pdb.set_trace()\n",
    "    \n",
    "    modelledData = df.loc[(df[\"x\"] == x) & (df[\"y\"] == y)]\n",
    "    \n",
    "    dfV = df2.loc[(df2[\"x\"] == x) & (df2[\"y\"] == y)]\n",
    "    \n",
    "    dfV['dates'] = pandas.to_datetime(dfV['dates'])\n",
    "    \n",
    "    modelledData['sDate'] = pandas.to_datetime(modelledData['sDate'])\n",
    "    \n",
    "    modelledData['eDate'] = pandas.to_datetime(modelledData['eDate'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return dfV, modelledData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#simple function to find the closest date in a list of dates, i.e. find the closest image date to a monthly rainfall date\n",
    "def nearestDate(base, dates):\n",
    "    \n",
    "    #pdb.set_trace()\n",
    "        \n",
    "    nearness = { abs(base - date) : date for date in dates }\n",
    "    \n",
    "    return nearness[min(nearness.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# function to apply a cross correlation to two datsets to determine the positive or negative shift required to align one data set to the other, uses a fast fourier transform.\n",
    "\n",
    "def cross_correlation_using_fft(x, y):\n",
    "\n",
    "    f1 = fft(x)\n",
    "    \n",
    "    f2 = fft(np.flipud(y))\n",
    "    \n",
    "    cc = np.real(ifft(f1 * f2))\n",
    "    \n",
    "    return fftshift(cc)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# function to compute the shift between the two datasets (npv and SPI)\n",
    "\n",
    "def compute_shift(x, y):\n",
    "    \n",
    "    assert len(x) == len(y)\n",
    "    \n",
    "    c = cross_correlation_using_fft(x, y) # calls the fft function to determine the lag between the two data sets\n",
    "    \n",
    "    assert len(c) == len(x)\n",
    "    \n",
    "    zero_index = int(len(x) / 2) - 1\n",
    "    \n",
    "    shift = zero_index - np.argmax(c)\n",
    "    \n",
    "    return shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#Function that takes both the input pixel spi and npv data and returns arrays of adjusted spi values and dates at npv peak locations.\n",
    "\n",
    "def doPeakAssessment(dfV,modelledData,spiSiteData):\n",
    "      \n",
    "    #find the peaks and troughs\n",
    "    \n",
    "    peakMask = dfV['peaks'] == True\n",
    "\n",
    "    peaks = dfV[peakMask]\n",
    "\n",
    "    troughMask = dfV['troughs'] == True\n",
    "\n",
    "    troughs = dfV[troughMask]\n",
    "    \n",
    "    #check the start and ends\n",
    "    pS= peaks['dates'].iloc[0]-troughs['dates'].iloc[0]\n",
    "\n",
    "    if pS.days >0:\n",
    "\n",
    "        peakStart = False\n",
    "\n",
    "    else:\n",
    "        peakStart = True\n",
    "\n",
    "    pE= peaks['dates'].iloc[-1]-troughs['dates'].iloc[-1]\n",
    "\n",
    "    if pE.days >0:\n",
    "\n",
    "        peakEnd = True\n",
    "    else:\n",
    "        peakEnd = False\n",
    "\n",
    "    #Four possible scenarios\n",
    "\n",
    "    #Scenario 1 is starts with trough and ends with trough\n",
    "\n",
    "    troughInd = troughs.index.tolist()\n",
    "\n",
    "    peaksInd = peaks.index.tolist()\n",
    "\n",
    "    if peakStart == False and peakEnd == False:\n",
    "\n",
    "        sliceIndex = troughInd\n",
    "\n",
    "    #scenario 2 is starts with peak and ends with peak\n",
    "\n",
    "    if peakStart == True and peakEnd == True:       \n",
    "\n",
    "        sliceIndex = troughInd\n",
    "\n",
    "        sliceIndex.insert(0, peaksInd[0]) # insert peak start index\n",
    "\n",
    "        sliceIndex.insert(len(sliceIndex), peaksInd[-1]) #inser peak end index\n",
    "\n",
    "    #scenario 3 is starts with peak and ends with trough\n",
    "\n",
    "    if peakStart == True and peakEnd == False:       \n",
    "\n",
    "        sliceIndex = troughInd\n",
    "\n",
    "        sliceIndex.insert(0, peaksInd[0]) # insert peak start index\n",
    "\n",
    "    #scenario 3 is starts with peak and ends with trough\n",
    "\n",
    "    if peakStart == False and peakEnd == True:       \n",
    "\n",
    "        sliceIndex = troughInd\n",
    "\n",
    "        sliceIndex.insert(len(sliceIndex), peaksInd[-1]) # insert peak start index\n",
    "\n",
    "    shifted = []\n",
    "\n",
    "    cycleLength = []\n",
    "\n",
    "    noYears = []\n",
    "\n",
    "    #loop through and slice the data into growth cycles of NPV, rainfall and spi\n",
    "    for i in range(len(sliceIndex)):\n",
    "\n",
    "        if i <len(sliceIndex)-1:\n",
    "            \n",
    "          \n",
    "            #slice NPV\n",
    "\n",
    "            start = sliceIndex[i]\n",
    "\n",
    "            end = sliceIndex[i+1]\n",
    "\n",
    "            cycle = dfV['npv'].iloc[start:end].tolist()\n",
    "            \n",
    "            cycleDates = dfV['dates'].iloc[start:end]\n",
    "            \n",
    "            #loop to get spi values at cycle dates\n",
    "               \n",
    "            spiSliced = []\n",
    "            \n",
    "            for z in cycleDates:\n",
    "                \n",
    "                d = nearestDate(z, spiSiteData['season'])\n",
    "                \n",
    "                mask = spiSiteData['season']== d\n",
    "\n",
    "                spiInd = mask[mask == True].index.tolist()\n",
    "                \n",
    "                spiSliced.append(spiSiteData['stats'].loc[spiInd[0]])\n",
    "            \n",
    "\n",
    "            #slice spi\n",
    "            startDate = dfV['dates'].iloc[start]\n",
    "\n",
    "            endDate = dfV['dates'].iloc[end]\n",
    "\n",
    "            matchS = nearestDate(startDate, spiSiteData['season'])\n",
    "\n",
    "            matchE = nearestDate(endDate, spiSiteData['season'])\n",
    "\n",
    "            dateDif = matchS - matchE\n",
    "\n",
    "            dateDif = abs(dateDif.days)\n",
    "\n",
    "            years = dateDif/365\n",
    "\n",
    "            time = np.arange(1-len(cycle),len(cycle))\n",
    "         \n",
    "          \n",
    "            shift = compute_shift(cycle, spiSliced)\n",
    "            \n",
    "            shifted.append(int(shift))\n",
    "\n",
    "            cycleLength.append(len(cycle))\n",
    "\n",
    "            noYears.append(years)   \n",
    "                      \n",
    "    abShifted = abs(np.array(shifted)) #convert to absolute values for testing\n",
    "\n",
    "    lengthCompare = abShifted > cycleLength #this is a test to check if the cross correlation has resulted in an error, i.e the lag is greater than the length of the cycle.\n",
    "\n",
    "    noYearsTest = np.array(noYears) > 3 #this is a test to find cycles with a length greater than three years, only peaks with a cycle greater than 3 years will be shifted.\n",
    "\n",
    "    peaksArray = np.array(peaksInd) #convert the original peakInd to an array\n",
    "    \n",
    "    peaksLength = peaksArray[lengthCompare] #return indexs of the peaks that have a greater cycle length than the calculated shift, i.e. lag error\n",
    "    \n",
    "    peaksYears = peaksArray[noYearsTest] #returns the indexes of peaks that have a cycle length greater than 3 years. These are the ones that we can apply the lag shift too.\n",
    "\n",
    "    shiftedArray = np.array(shifted) #convert to np array\n",
    "    \n",
    "    shiftedYears = shiftedArray[noYearsTest] #returns the lag for only the peaks > 3 years\n",
    "\n",
    "    #Loop to go through and replace shiftedYears values with a zero that have been filtered on years but in the rare case have a lag error.\n",
    "    for i in peaksLength:\n",
    "\n",
    "        comp = np.where(i== peaksYears) #search peaks years for a peaks length value and return its index\n",
    "\n",
    "        comp = np.array(comp) #convert to a np array\n",
    "\n",
    "\n",
    "        if comp.size:\n",
    "\n",
    "            comp = comp[0] \n",
    "\n",
    "            shiftedYears[comp] = 0 #replace the error with a zero\n",
    "        \n",
    "    #Do the adjustment for lag\n",
    "    \n",
    "    adjusted = peaksYears+shiftedYears\n",
    "    \n",
    "    adjusted4Plot = peaksArray + shiftedArray\n",
    "    \n",
    "    for n in range(len(adjusted4Plot)):\n",
    "        \n",
    "        if adjusted4Plot[n] > len(dfV)-1:\n",
    "            \n",
    "            adjusted4Plot[n]=len(dfV)-1\n",
    "                        \n",
    "        if adjusted4Plot[n] < 0:\n",
    "                   \n",
    "            adjusted4Plot[n] = 0\n",
    "   \n",
    "    #loop to get the spi values from the adjusted index's\n",
    "\n",
    "    adjustedDates = dfV['dates'].iloc[adjusted]\n",
    "    \n",
    "    adjustedDates4Plot = dfV['dates'].iloc[adjusted4Plot]\n",
    "\n",
    "    adjustedSpi = []\n",
    "    \n",
    "    adjustedSpiDate = []\n",
    "    \n",
    "    adjustedSpi4Plot = []\n",
    "    \n",
    "    adjustedSpiDate4Plot = []\n",
    "    \n",
    "    for i in adjustedDates:\n",
    "\n",
    "        match = nearestDate(i, spiSiteData['season'])\n",
    "\n",
    "        mask = spiSiteData['season']== match\n",
    "\n",
    "        spi = spiSiteData[mask]\n",
    "\n",
    "        adjustedSpi.append(spi['stats'].iloc[0])\n",
    "        \n",
    "        adjustedSpiDate.append(spi['season'].iloc[0])\n",
    "        \n",
    "    for i in adjustedDates4Plot:\n",
    "\n",
    "        match = nearestDate(i, spiSiteData['season'])\n",
    "\n",
    "        mask = spiSiteData['season']== match\n",
    "\n",
    "        spi = spiSiteData[mask]\n",
    "\n",
    "        adjustedSpi4Plot.append(spi['stats'].iloc[0])\n",
    "        \n",
    "        adjustedSpiDate4Plot.append(spi['season'].iloc[0])\n",
    "    \n",
    "    #loop to get the spi values from the peak index without the lag adjustment \n",
    "\n",
    "    noNadjustedDates = dfV['dates'].iloc[peaksInd]  \n",
    "      \n",
    "    noNadjustedSpi = []\n",
    "\n",
    "    noNadjustedSpiDate = []\n",
    "    \n",
    "    for i in noNadjustedDates:\n",
    "\n",
    "        match = nearestDate(i, spiSiteData['season'])\n",
    "\n",
    "        mask = spiSiteData['season']== match\n",
    "\n",
    "        spi = spiSiteData[mask]\n",
    "\n",
    "        noNadjustedSpi.append(spi['stats'].iloc[0])\n",
    "        \n",
    "        noNadjustedSpiDate.append(spi['season'].iloc[0])\n",
    "\n",
    "    return adjustedSpi,adjustedSpiDate,noNadjustedSpi,noNadjustedSpiDate,adjustedSpi4Plot,adjustedSpiDate4Plot   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# function to process all of the pixels or at least a subset of the 2km x 2km image chip\n",
    "\n",
    "def doDoAllSpiAssess(period):\n",
    "    \n",
    "    #get data and arrange\n",
    "             \n",
    "    spiSiteData = getSpiData(text.value,period) # get the spi site data, this is as at the site coord, havent averaged anything across the chip i.e. 20 x 20 due to the course nature of the rainfall grids.\n",
    "    \n",
    "    #npv,profile = openTif('data/chip.tif') #get npv data\n",
    "    \n",
    "    #(nBands,nRows,nCols) = npv.shape\n",
    "    \n",
    "    #temp = pandas.read_csv('data/dates.csv',header =0,parse_dates=['0'],dayfirst=True) #reads the csv into a pandas array\n",
    "    \n",
    "    #dates = pandas.DataFrame()\n",
    "    \n",
    "    #dates['dates']=temp['0']\n",
    "    \n",
    "    # Loop through the image chip and do the peak assessment\n",
    "         \n",
    "    allAdjustedSpi = []\n",
    "\n",
    "    allNoNadjustedSpi = []\n",
    "    \n",
    "    allAdjustedSpi4Plot = []\n",
    "    \n",
    "    f = FloatProgress(min=0, max=67/3)\n",
    "    \n",
    "    display(f)#progress bar\n",
    "\n",
    "    for i in range(67/3): # note: I have divided by three to reduce the number of pixels i.e. a subset of the image chip, this is to reduce the processing time\n",
    "                        \n",
    "        print str(i+1) + \" of \" +str(67/3)\n",
    "        \n",
    "        f.value = i\n",
    "                \n",
    "        ff = FloatProgress(min=0, max=67/3)\n",
    "    \n",
    "        display(ff)#progress bar\n",
    "        \n",
    "        for ii in range(67/3): \n",
    "            \n",
    "            ff.value = ii     \n",
    "                       \n",
    "            dfV,modelledData = getOutputData(\"data/output.csv\",'data/outputTimeSeriesPk.pkl',int(i),int(ii))\n",
    "            \n",
    "            adjustedSpi,adjustedSpiDate,noNadjustedSpi,noNadjustedSpiDate,adjustedSpi4Plot,adjustedSpiDate4Plot  = doPeakAssessment(dfV,modelledData,spiSiteData)\n",
    "            \n",
    "            for j in adjustedSpi:\n",
    "\n",
    "                allAdjustedSpi.append(j)\n",
    "\n",
    "            for k in noNadjustedSpi:\n",
    "\n",
    "                allNoNadjustedSpi.append(k)\n",
    "                \n",
    "            for l in adjustedSpi4Plot:\n",
    "                \n",
    "                allAdjustedSpi4Plot.append(l)\n",
    "                \n",
    "    return allAdjustedSpi, allNoNadjustedSpi, allAdjustedSpi4Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# function to loop through a number of defined spi periods and calculate the shift, used namely in the testing phase, have hard wired to 36 months for the arid study area\n",
    "\n",
    "def doSpiProcess(btn):\n",
    "            \n",
    "    print 'Processing started.....'\n",
    "    \n",
    "    print \"Note: this can take up to 20 minutes\"\n",
    "            \n",
    "    #periods = [\"6m\",\"12m\",\"24m\",\"36m\"] # this could be used to select other periods, i.e. for more seasonally driven sytems\n",
    "    \n",
    "    periods = [\"36m\"]\n",
    "            \n",
    "    results = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(periods)):           \n",
    "\n",
    "            allAdjustedSpi, allNoNadjustedSpi, allAdjustedSpi4Plot  = doDoAllSpiAssess(periods[i])\n",
    "\n",
    "            tempDF = pd.DataFrame(allAdjustedSpi,columns=['adjusted_'+periods[i]])\n",
    "\n",
    "            results = pd.concat([results,tempDF])\n",
    "\n",
    "            tempDF = pd.DataFrame(allNoNadjustedSpi,columns=['raw_'+periods[i]])\n",
    "\n",
    "            results = pd.concat([results,tempDF])\n",
    "            \n",
    "            tempDF = pd.DataFrame(allAdjustedSpi4Plot,columns=['adjusted4Plot_'+periods[i]])\n",
    "\n",
    "            results = pd.concat([results,tempDF])\n",
    "\n",
    "    cols =  results.columns \n",
    "    \n",
    "    results.to_csv(\"data/spiPeakResult.csv\") #write to output csv\n",
    "    \n",
    "    #next stage is classify the spi data into the following wet / dry catergories\n",
    "    \n",
    "    '''\n",
    "    Based on Shah etal 2015 'Drought Index Computation Using Standardized Precipitation Index (SPI) Method For Surat District, Gujarat'  SPI catergories:\n",
    "    \n",
    "        SPI Range Category\n",
    "    + 2 to more Extremely wet\n",
    "    1.5 to 1.99 Very wet\n",
    "    1.0 to 1.49 Moderately wet\n",
    "    -0.99 to 0.99 Near normal\n",
    "    -1.0 to -1.49 Moderately dry\n",
    "    -1.5 to -1.99 Severely dry\n",
    "    -2 to less Extremely dry    \n",
    "    '''\n",
    "    spiInd = [-2,-2,-1.5,-1,1.0,1.5,2.0]\n",
    "       \n",
    "    allCats = []\n",
    "    \n",
    "    # loop through spi values and catergorise\n",
    "\n",
    "    for i in cols:\n",
    "\n",
    "        m = results[i]\n",
    "\n",
    "        notNulls = pd.notnull(m)\n",
    "\n",
    "        sizeM = sum(notNulls)\n",
    "\n",
    "        cats = []\n",
    "\n",
    "        for ii in range(len(spiInd)):\n",
    "\n",
    "            if ii == 0:\n",
    "\n",
    "                sliced = m[notNulls]\n",
    "\n",
    "                testA = sliced<spiInd[ii]\n",
    "\n",
    "                resultA = sliced[testA]\n",
    "\n",
    "                resSum = len(resultA)\n",
    "\n",
    "                perc = float(resSum)/float(sizeM)*100\n",
    "\n",
    "                cats.append(perc)\n",
    "\n",
    "            elif ii == len(spiInd)-1:\n",
    "\n",
    "                sliced = m[notNulls]\n",
    "\n",
    "                testA = sliced>spiInd[ii]\n",
    "\n",
    "                resultA = sliced[testA]\n",
    "\n",
    "                resSum = len(resultA)\n",
    "\n",
    "                perc = float(resSum)/float(sizeM)*100\n",
    "\n",
    "                cats.append(perc)\n",
    "\n",
    "            else:            \n",
    "\n",
    "                sliced = m[notNulls]\n",
    "\n",
    "                testA = sliced>spiInd[ii]\n",
    "\n",
    "                resultA = sliced[testA]\n",
    "\n",
    "                testB = resultA<spiInd[ii+1]\n",
    "\n",
    "                resultB = resultA[testB]\n",
    "\n",
    "                resSum = len(resultB)\n",
    "\n",
    "                perc = float(resSum)/float(sizeM)*100\n",
    "\n",
    "                cats.append(perc)\n",
    "\n",
    "        allCats.append(cats) \n",
    "\n",
    "    resultAllCats = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(allCats)):\n",
    "\n",
    "        resultAllCats[cols[i]]=allCats[i]\n",
    "        \n",
    "    print 'Processing complete'\n",
    "\n",
    "    resultAllCats.to_csv(\"data/spiPeakAssessment.csv\") #write to output csv\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 17,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n"
     ]
    }
   ],
   "source": [
    "btn = widgets.Button(description=\"Evalute peak detection!\")\n",
    "btn.on_click(doSpiProcess)\n",
    "display(btn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing started.....\n",
      "Note: this can take up to 20 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f335361e70dc4802809954f2d729b143"
      }
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50cef48de0cc4d04bc652d72eb78960c"
      }
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4c05e6f3c703>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoSpiProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-94bd0e82cb04>\u001b[0m in \u001b[0;36mdoSpiProcess\u001b[0;34m(btn)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperiods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mallAdjustedSpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallNoNadjustedSpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallAdjustedSpi4Plot\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdoDoAllSpiAssess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperiods\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mtempDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallAdjustedSpi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'adjusted_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mperiods\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-31e6552311e7>\u001b[0m in \u001b[0;36mdoDoAllSpiAssess\u001b[0;34m(period)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mdfV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodelledData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetOutputData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/output.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'data/outputTimeSeriesPk.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0madjustedSpi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0madjustedSpiDate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnoNadjustedSpi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnoNadjustedSpiDate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0madjustedSpi4Plot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0madjustedSpiDate4Plot\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdoPeakAssessment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodelledData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspiSiteData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0madjustedSpi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-f2e7d28e1f21>\u001b[0m in \u001b[0;36mdoPeakAssessment\u001b[0;34m(dfV, modelledData, spiSiteData)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcycleDates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnearestDate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspiSiteData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'season'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspiSiteData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'season'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-3d39c1f30ed1>\u001b[0m in \u001b[0;36mnearestDate\u001b[0;34m(base, dates)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnearness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdates\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnearness\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnearness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "doSpiProcess('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 21,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "#### Plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 21,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "Creates a BAR plot of the proportion of peaks detecting in each spi catergory for both lag adjusted spi, non-lag adjusted spi for both major and all growth cycles (major >3 years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dospiAssessPlot(b):\n",
    "\n",
    "    temp = pandas.read_csv('data/spiPeakAssessmentAllSites.csv',header =0) #import the output from the assessment (see methods section)\n",
    "    \n",
    "    columns = temp.keys()\n",
    "    \n",
    "    #pdb.set_trace()\n",
    "    \n",
    "    columns = columns[1:3]\n",
    "    \n",
    "    temp = temp[columns]\n",
    "    \n",
    "    #Grouping of Shah etal 2015 catergories\n",
    "    \n",
    "    normToWet = temp.iloc[3:] # Normal -1 to 1 to extremely wet +2\n",
    "\n",
    "    normToWetSums = []\n",
    "        \n",
    "    cols = temp.columns\n",
    "    \n",
    "    cols = ['All years','Long term (>3 years)']\n",
    "    \n",
    "    # loop through and create bar chart series for each catergory of the proportions of peak detections.\n",
    "         \n",
    "    data = []  \n",
    "    \n",
    "    names = ['Extremely dry','Severely dry','Moderately dry','Near normal','Moderately wet','Very wet','Extremely wet']\n",
    "    \n",
    "    for i in range(len(temp)):\n",
    "    \n",
    "        x = cols\n",
    "        \n",
    "        y = temp.iloc[i]\n",
    "        \n",
    "        r = 255-i*30\n",
    "        \n",
    "        g = 255-i*30\n",
    "        \n",
    "        b = 255-i*30\n",
    "        \n",
    "        colour = \"rgb(\" + str(r)+','+str(g)+','+str(b)+')'\n",
    "        \n",
    "        trace = go.Bar(x=x,y=y,name=names[i],marker=dict(color=colour))\n",
    "    \n",
    "        data.append(trace)\n",
    "    \n",
    "    \n",
    "    layout = go.Layout(barmode='group',title='Proportions of peaks detected in each Standardised Precipitation Index (SPI) Catergory',xaxis=dict(title='Standardised Precipitation Index Catergories'),yaxis=dict(title='Proportion %'))\n",
    "    \n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    \n",
    "    spiPlot = py.iplot(fig, filename='grouped-bar')\n",
    "    \n",
    "    display(spiPlot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed properly. Did you enable the widgetsnbextension? If not, then run \"jupyter nbextension enable --py --sys-prefix widgetsnbextension\"\n"
     ]
    }
   ],
   "source": [
    "btn = widgets.Button(description=\"Display peak assessment plot!\")\n",
    "btn.on_click(dospiAssessPlot)\n",
    "display(btn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~NTPlotly/961.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dospiAssessPlot('b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 25,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "Create a plot of both NPV and SPI with associated spi peak adjusted points at both all peaks and only peask with a growth period greater than three years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def doDeficitPlot(peaks,troughs,data,spiSiteData,adjustedSpi,adjustedSpiDates,adjustedSpi4plot,adjustedSpiDates4Plot):\n",
    "    \n",
    "    #pdb.set_trace()\n",
    "    \n",
    "    peakSpi = []\n",
    "    \n",
    "    peakSpiDates = []\n",
    "    \n",
    "    troughSpi = []\n",
    "    \n",
    "    troughSpiDates = []\n",
    "    \n",
    "    for i in peaks['dates']:\n",
    "\n",
    "        match = nearestDate(i, spiSiteData['season'])\n",
    "\n",
    "        mask = spiSiteData['season']== match\n",
    "\n",
    "        spi = spiSiteData[mask]\n",
    "\n",
    "        peakSpi.append(spi['stats'].iloc[0])\n",
    "\n",
    "        peakSpiDates.append(spi['season'].iloc[0])\n",
    "\n",
    "    for i in troughs['dates']:\n",
    "\n",
    "        match = nearestDate(i, spiSiteData['season'])\n",
    "\n",
    "        mask = spiSiteData['season']== match\n",
    "\n",
    "        spi = spiSiteData[mask]\n",
    "\n",
    "        troughSpi.append(spi['stats'].iloc[0])\n",
    "\n",
    "        troughSpiDates.append(spi['season'].iloc[0])\n",
    "    \n",
    "    \n",
    "    spiAdjPeak4Plot = go.Scatter(x=adjustedSpiDates4Plot,y=adjustedSpi4plot,name='SPI-Peak All (lag)',mode = 'markers', marker = dict(color = ('rgb(255, 255, 0)'),size = 14,symbol='circle'),yaxis='y2')\n",
    "\n",
    "    data.append(spiAdjPeak4Plot)\n",
    "    \n",
    "    spiAdjPeak = go.Scatter(x=adjustedSpiDates,y=adjustedSpi,name='SPI-Peak (lag)',mode = 'markers', marker = dict(color = ('rgb(255, 0, 0)'),size = 14,symbol='circle'),yaxis='y2')\n",
    "\n",
    "    data.append(spiAdjPeak)\n",
    "    \n",
    "    \n",
    "    spiPeak = go.Scatter(x=peakSpiDates,y=peakSpi,name='SPI-Peak',mode = 'markers', marker = dict(color = ('rgb(0, 0, 255)'),size = 8,symbol='circle'),yaxis='y2')\n",
    "\n",
    "    data.append(spiPeak)\n",
    "   \n",
    "\n",
    "    allSpi = go.Scatter(x=spiSiteData['season'],y=spiSiteData['stats'],name='SPI',mode = 'lines', marker = dict(color = ('rgb(0, 0, 255)'),size = 8,symbol='circle'),yaxis='y2')\n",
    "\n",
    "    data.append(allSpi)\n",
    "\n",
    "    layout = go.Layout(yaxis=dict(title='Non-Photosynthetic Vegetation Cover %'),xaxis=dict(title='Time'),font=dict(family='Arial, monospace', size=14, color = ('rgb(1, 1, 1)')),yaxis2=dict(title='Standardised Precipitation Index',titlefont=dict(color='rgb(148, 103, 189)'),tickfont=dict(color='rgb(148, 103, 189)'),overlaying='y',side='right'))\n",
    "\n",
    "    fig = go.Figure(data=data,layout=layout)\n",
    "\n",
    "    filename1 = \"researchProject/spi\"\n",
    "\n",
    "    plot = py.iplot(fig, filename=filename1)       \n",
    "    \n",
    "    return  plot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#Function that gets both the npv and spi data once the button is clicked\n",
    "\n",
    "def on_button_clicked_spi(b):\n",
    "           \n",
    "    site = text.value\n",
    "  \n",
    "    temp1 = temp2.value\n",
    "    \n",
    "    spiSiteData = getSpiData(site,temp1)\n",
    "    \n",
    "    r = row.value\n",
    "    \n",
    "    c = col.value\n",
    "    \n",
    "    temp = pandas.read_csv('data/dates.csv',header =0,parse_dates=['0'],dayfirst=True) #reads the csv into a pandas array\n",
    "    \n",
    "    dates = pandas.DataFrame()\n",
    "    \n",
    "    dates['dates']=temp['0']\n",
    "    \n",
    "    dfV,modelledData = getOutputData(\"data/output.csv\",'data/outputTimeSeriesPk.pkl',int(r),int(c))\n",
    "\n",
    "    peakMask = dfV['peaks'] == True\n",
    "\n",
    "    peaks = dfV[peakMask]\n",
    "\n",
    "    troughMask = dfV['troughs'] == True\n",
    "\n",
    "    troughs = dfV[troughMask]\n",
    "\n",
    "    locFig,data,layout = doLocationPlots(dfV,modelledData)\n",
    "    \n",
    "    #pdb.set_trace()\n",
    "\n",
    "    adjustedSpi,adjustedSpiDates,noNadjustedSpi,noNadjustedSpiDates,adjustedSpi4Plot,adjustedSpiDates4Plot = doPeakAssessment(dfV,modelledData,spiSiteData)\n",
    "    \n",
    "    st = dates.iloc[0]\n",
    "    \n",
    "    st = st.iloc[0]\n",
    "    \n",
    "    match = nearestDate(st, spiSiteData['season'])\n",
    "\n",
    "    mask = spiSiteData['season']>= match\n",
    "\n",
    "    spiSiteData1 = spiSiteData[mask]\n",
    "\n",
    "    plot = doDeficitPlot(peaks,troughs,data,spiSiteData1,adjustedSpi,adjustedSpiDates,adjustedSpi4Plot,adjustedSpiDates4Plot)  \n",
    "        \n",
    "    display(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def doLocationPlots(df,modelledData):\n",
    "     \n",
    "    \n",
    "    newDf = df.set_index(['dates'])\n",
    "    \n",
    "    sDateSliced = newDf.ix[modelledData['sDate']]\n",
    "        \n",
    "    eDateSliced = newDf.ix[modelledData['eDate']] \n",
    "   \n",
    "        \n",
    "    trace1=[]\n",
    "    \n",
    "    trace1a = go.Scatter(x=df['dates'],y=df['fittedSpline']-100.0,name='NPV Spline',mode = 'lines', line = dict(color = ('rgb(1, 1, 1)'),width = 2))\n",
    "    \n",
    "    trace1.append(trace1a)\n",
    "    \n",
    "    trace1b = go.Scatter(x=modelledData['sDate'],y=sDateSliced['fittedSpline']-100.0,name='Peaks',mode = 'markers', marker = dict(color = ('rgb(1, 1, 1)'),size = 13,symbol='circle'))\n",
    "    \n",
    "    trace1.append(trace1b)\n",
    "    \n",
    "    trace1c = go.Scatter(x=modelledData['eDate'],y=eDateSliced['fittedSpline']-100.0,name='Troughs',mode = 'markers', marker = dict(color = ('rgb(1, 1, 1)'),size = 13,symbol='triangle-up'))\n",
    "   \n",
    "    trace1.append(trace1c)\n",
    "    \n",
    "    trace1d = go.Scatter(x=df['dates'],y=df['npv']-100.0,name='NPV',mode = 'markers', marker = dict(color = ('rgb(1, 1, 1)'),size = 1,symbol='circle'))\n",
    "    \n",
    "    trace1.append(trace1d)\n",
    "            \n",
    "    layout = go.Layout(yaxis=dict(title='Non-Photosynthetic Vegetation Cover %'),xaxis=dict(title='Time'),font=dict(family='Arial, monospace', size=14, color = ('rgb(1, 1, 1)')))\n",
    "      \n",
    "    fig = go.Figure(data=trace1, layout=layout)\n",
    "        \n",
    "    #peakUrl = py.iplot(fig,filename=filename)   \n",
    "    \n",
    "    #pdb.set_trace()\n",
    "    \n",
    "    return fig,trace1,layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 25,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "row = widgets.Text(description=\"y:(0-67)\")\n",
    "col = widgets.Text(description=\"x:(0-67)\")\n",
    "row.value = '33'\n",
    "col.value = '33'\n",
    "display(row)\n",
    "display(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "temp2 = widgets.Dropdown(options ={'6 months':'6m','12 months':'12m','24 months':'24m','36 months':'36m'}) # default is 36 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 25,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "display(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 29,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "button = widgets.Button(description=\"Rainfall deficit\")\n",
    "display(button)\n",
    "button.on_click(on_button_clicked_spi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 29,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### NPV Ground Cover - Peak Detection Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
