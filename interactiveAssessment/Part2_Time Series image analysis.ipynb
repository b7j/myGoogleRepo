{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 2,
        "hidden": false,
        "row": 0,
        "width": 3
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "### Part 2. Time series analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": true,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "##### Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install utm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install cufflinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "#interactive web map package\n",
    "import folium \n",
    "from folium import features\n",
    "import shapefile\n",
    "from json import dumps\n",
    "from folium import plugins\n",
    "\n",
    "#Miscellanous\n",
    "from IPython.core.display import display #display inline package\n",
    "from ipywidgets import interact, interactive, fixed, FloatProgress\n",
    "import ipywidgets as widgets\n",
    "import pdb #debugging tool\n",
    "import pandas as pd # pandas dataframe package\n",
    "import pandas\n",
    "from datetime import datetime #date conversion tool\n",
    "from xlrd.xldate import xldate_as_tuple #xldate converter\n",
    "from urllib2 import urlopen #get data from web tool\n",
    "#from collections import OrderedDict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.display import Image\n",
    "import utm #lat long to utms\n",
    "import zipfile #unzip tool\n",
    "from IPython.display import clear_output\n",
    "from scipy.signal import correlate #rainfall correlation\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "#numpy packages and tools\n",
    "import numpy as np\n",
    "from numpy import linspace\n",
    "from numpy.fft import fft, ifft, fft2, ifft2, fftshift\n",
    "\n",
    "#Spatial data packages\n",
    "import rasterio #io raster data \n",
    "#from osgeo import gdal\n",
    "\n",
    "#Plotly tools\n",
    "import plotly.tools as tls\n",
    "tls.set_credentials_file(username='NTPlotly', api_key='48kd2al3f2') #my plotly credentials (please dont use they cost me $$)\n",
    "import plotly.plotly as py\n",
    "#from plotly.graph_objs import *\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "import cufflinks as cf\n",
    "import matplotlib\n",
    "\n",
    "#scipy packages\n",
    "from scipy.misc import derivative #derivates of functions tool\n",
    "from scipy.interpolate import UnivariateSpline #basic spline fitting tool\n",
    "from scipy.signal import gaussian #gaussian filtering tool\n",
    "from scipy.ndimage import filters #anotehr filtering tool\n",
    "from scipy import stats #core scipy stats package\n",
    "from scipy.integrate import simps #simpsons rule package for trapezoid calcualtions in area under curve calcs\n",
    "\n",
    "#Basic math tools\n",
    "from math import log\n",
    "from math import factorial\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": true,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "### Spline fitting and crossing tests to detect and map peaks and troughs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#simple function to call moving average function\n",
    "def doParameters(y):\n",
    "        \n",
    "    avg, param = movingAverage(y)\n",
    "           \n",
    "    return param    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#Gaussian type filter to find both the average and the variance of the raw data.\n",
    "def movingAverage(series, sigma=3):\n",
    "\n",
    "    b = gaussian(39, sigma)\n",
    "    \n",
    "    average = filters.convolve1d(series, b/b.sum())\n",
    "    \n",
    "    var = filters.convolve1d(np.power(series-average,2), b/b.sum())\n",
    "    \n",
    "    return average, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": true,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "##### Spline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": true,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "A univariate spline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def doSpline(x,y,paramVar,paramMan):\n",
    "    \n",
    "    #pdb.set_trace()\n",
    "    \n",
    "    spl1 = UnivariateSpline(x, y, w=0.9/np.sqrt(paramVar)) # weighting factor method\n",
    "    #spl1 = UnivariateSpline(x, y) # weighting factor method   \n",
    "    \n",
    "    spl2 = UnivariateSpline(x, y,s=paramMan) #manual smoothing method\n",
    "    \n",
    "        \n",
    "    return spl1,spl2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": true,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "##### Derivative estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": true,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "Estimates the 1st derivative / slope of the spline function at each data point and appends to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def findDerivatives(spl,x):\n",
    "    \n",
    "    splDeriv = []\n",
    "       \n",
    "    for i in x:\n",
    "\n",
    "        a = spl.derivatives(i)\n",
    "\n",
    "        splDeriv.append(a[1])\n",
    "    \n",
    "    \n",
    "    return splDeriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": true,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "##### Crossing axis test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": true,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "Simple test to find changes in the sign of the derivate estimations as crossing points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def crossingTest(first):\n",
    "\n",
    "    peakCrossing = np.zeros(len(first))\n",
    "\n",
    "    troughCrossing = np.zeros(len(first))\n",
    "\n",
    "    for i in range(len(first)):\n",
    "\n",
    "        if i >0:\n",
    "\n",
    "            d1 = first[i-1]\n",
    "\n",
    "            d2 = first[i]\n",
    "            \n",
    "            d1s = sign(d1)\n",
    "\n",
    "            d2s = sign(d2)\n",
    "\n",
    "            if d1s > d2s:\n",
    "\n",
    "                peakCrossing[i]=1\n",
    "\n",
    "            elif d1s < d2s:\n",
    "\n",
    "                troughCrossing[i]=1\n",
    "\n",
    "    return peakCrossing, troughCrossing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#basic sign test\n",
    "def sign(x): \n",
    "\n",
    "    return 1 if x >= 0 else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": true,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "##### Main peak to trough function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": true,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "Function that takes the raw data inputs applies the spline and derivate functions, returns peaks, troughs for both manual and weighted parameterisation methods and returns the zero filtered raw data for further analyses and plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def doDerivativePeakTrough(data,dates,nBands):\n",
    "\n",
    "    zeros = data > 0 #create a mask of the zeros to remove them\n",
    "\n",
    "    y = data[zeros] #apply the zero mask to the data\n",
    "            \n",
    "    dates = np.array(dates) #turn dates into a numpy array\n",
    "    \n",
    "    newDates = dates[zeros]#apply the same zero mask to the data\n",
    "\n",
    "    x = linspace(1,len(y),len(y)) #create a new x axis from the data with the zeros removed\n",
    "   \n",
    "    paramVar = doParameters(y) # call a function that sets weighting factor based on the variance of the raw data.\n",
    "    \n",
    "    paramMan = 0.1 # set the weighting factor manually\n",
    "    \n",
    "    splVar,splMan= doSpline(x,y,paramVar,paramMan) #call fitted spline function and return models for both manual and variance fitting methods\n",
    "        \n",
    "    splDataV = splVar(x) #returns y spline fitted data (variance fit method)\n",
    "    \n",
    "    splDataM = splMan(x) #returns y spline fitted data (manual fit method)\n",
    "         \n",
    "    splDerivVar = findDerivatives(splVar,x) #return the first derivate of the spline function (variance fit method)\n",
    "    \n",
    "    splDerivMan = findDerivatives(splMan,x) #return the first derivate of the spline function (manual fit method)\n",
    "    \n",
    "    peakCrossingVar,troughCrossingVar = crossingTest(splDerivVar) # performs a crossing test to determine peaks and troughs  (variance fit method)\n",
    "    \n",
    "    peakCrossingMan,troughCrossingMan = crossingTest(splDerivMan) # performs a crossing test to determine peaks and troughs (manual fit method)\n",
    "\n",
    "    peaksVar = peakCrossingVar == 1 #filter crossing test result to index the peaks (variance fit method)\n",
    "\n",
    "    troughsVar = troughCrossingVar == 1 #filter crossing test to index the troughs (variance fit method)\n",
    "    \n",
    "    peaksMan = peakCrossingMan == 1  #filter crossing test result to index the peaks (manual fit method)\n",
    "\n",
    "    troughsMan = troughCrossingMan == 1 #filter crossing test to index the troughs (manual fit method)\n",
    "    \n",
    "    return peaksVar,troughsVar,peaksMan,troughsMan,splDataV, splDataM,newDates,y,splDerivVar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": true,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "### Determining the utilisation periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": true,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "This section aims to identify the utilisation periods in the time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": true,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "##### Organise data into a pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": true,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "Takes the results from the peak and trough detection and organises into a new data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def doPandasDf(peaksVar,troughsVar,splDataV,newDates,newData,firstDeriv):\n",
    "    \n",
    "    #pdb.set_trace()\n",
    "                    \n",
    "    df = pandas.DataFrame(newDates,columns=['dates'])\n",
    "            \n",
    "    df['dates'] = pandas.to_datetime(newDates)\n",
    "    \n",
    "    df['peaks'] = peaksVar\n",
    "\n",
    "    df['troughs']= troughsVar\n",
    "\n",
    "    df['npv'] = newData\n",
    "\n",
    "    df['fittedSpline'] = splDataV\n",
    "    \n",
    "    df['firstDeriv'] = firstDeriv\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": true,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "##### Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 5,
        "hidden": true,
        "row": 0,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "Function that checks for exceptions to expected ground cover dynamic's. For example heavilly timbered areas, persistant bare areas such as scalds, pans, dry lakes, timbered and non-timbered hills and mountains. A null value is returned where these areas are encountered. \n",
    "\n",
    "Utilisation modelling is then applied to areas not masked.\n",
    "\n",
    "Note: this is very brutal at this stage and will likely need refinement!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def doChecking(peakVals,troughVals,df,nBands):\n",
    "    \n",
    "    sDate = []\n",
    "\n",
    "    eDate = []\n",
    "    \n",
    "    slopes = []\n",
    "\n",
    "    auc = []\n",
    "\n",
    "    rSqr = []\n",
    "    \n",
    "    slopesF = []\n",
    "\n",
    "    aucF = []\n",
    "    \n",
    "    aucS = []\n",
    "\n",
    "    rSqrF = []\n",
    "\n",
    "    imageCount = []\n",
    "    \n",
    "    npv = []\n",
    "    \n",
    "    fitted = []\n",
    "\n",
    "    troughFlag = 0\n",
    "\n",
    "    peakFlag = 0   \n",
    "    \n",
    "    #pdb.set_trace()\n",
    "    \n",
    "    ## Filtering process\n",
    "    \n",
    "    if len(troughVals)>2 and len(peakVals) >2: #this filters only where more than one peak and trough exist.\n",
    "\n",
    "        startDif = troughVals.index[0] - peakVals.index[0] \n",
    "\n",
    "        if startDif <0:# this checks and corrects for when a trough is encountered prior to a peak at the start of the time series\n",
    "\n",
    "            troughVals = troughVals.iloc[1:]#this slices the ts from the second item to the end.\n",
    "\n",
    "            troughFlag =1\n",
    "\n",
    "        endDif =  troughVals.index[-1] - peakVals.index[-1]\n",
    "        \n",
    "        if endDif <0:#this checks and corrects for when a peak at the end is not proceeded by a trough. The previous peak is used.\n",
    "\n",
    "            peakVals = peakVals.iloc[:-1]#this slices the trough list from the beginning to the second last.\n",
    "\n",
    "            peakFlag = 1\n",
    "\n",
    "        for i in range(len(peakVals)):\n",
    "\n",
    "                start = peakVals.index[i]\n",
    "\n",
    "                startD = peakVals['dates'].iloc[i]\n",
    "\n",
    "                startD = startD.to_pydatetime()\n",
    "                \n",
    "                #pdb.set_trace()\n",
    "\n",
    "                #startD = startD.toordinal() #Commented this out for graphing purposes to use in output image need to uncomment\n",
    "\n",
    "                end = troughVals.index[i]\n",
    "\n",
    "                endD = troughVals['dates'].iloc[i]\n",
    "\n",
    "                endD = endD.to_pydatetime()\n",
    "\n",
    "                #endD = endD.toordinal() #Commented this out for graphing purposes to use in output image need to uncomment\n",
    "                \n",
    "                npvTemp = peakVals['npv'].iloc[i]\n",
    "                \n",
    "                fitTemp = npvTemp = peakVals['fittedSpline'].iloc[i]\n",
    "\n",
    "                if start < end:\n",
    "                    \n",
    "                    slope,r_value,area,t = doModellingRaw(df,start,end)\n",
    "                    \n",
    "                    slopeF,r_valueF,areaF,areaSmall = doModellingFitted(df,start,end)\n",
    "\n",
    "                    sDate.append(startD)\n",
    "\n",
    "                    eDate.append(endD)\n",
    "\n",
    "                    slopes.append(slope)\n",
    "\n",
    "                    auc.append(area)\n",
    "\n",
    "                    rSqr.append(r_value**2)\n",
    "                    \n",
    "                    slopesF.append(slopeF)\n",
    "\n",
    "                    aucF.append(areaF)\n",
    "                    \n",
    "                    aucS.append(areaSmall)\n",
    "\n",
    "                    rSqrF.append(r_valueF**2)\n",
    "\n",
    "                    imageCount.append(len(t))\n",
    "                    \n",
    "                    npv.append(npvTemp)\n",
    "                    \n",
    "                    fitted.append(fitTemp)\n",
    "\n",
    "    else:\n",
    "\n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        #cf.set_config_file(offline=False, world_readable=True, theme='ggplot')\n",
    "\n",
    "        #df.iplot(kind='scatter', mode='lines', x='dates', y='fittedSpline', filename='cufflinks/simple-scatter')\n",
    "        \n",
    "        zeros = [0] * 300\n",
    "\n",
    "        sDate = zeros\n",
    "\n",
    "        eDate = zeros\n",
    "\n",
    "        slopes = zeros\n",
    "\n",
    "        auc = zeros\n",
    "\n",
    "        rSqr = zeros\n",
    "        \n",
    "        slopesF = zeros\n",
    "\n",
    "        aucF = zeros\n",
    "        \n",
    "        aucS = zeros\n",
    "\n",
    "        rSqrF = zeros\n",
    "\n",
    "        imageCount = zeros\n",
    "        \n",
    "        npv = zeros\n",
    "        \n",
    "        fitted = zeros\n",
    "        \n",
    "    return sDate,eDate,slopes,auc,rSqr,slopesF,aucF,aucS,rSqrF,imageCount,npv,fitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": true,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "#####  Model slope and area under the curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": true,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "###### Area under curve and slope model (actual data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def doModellingRaw(df,start,end):\n",
    "        \n",
    "    npvRaw = df['npv'].loc[start:end]\n",
    "    \n",
    "    t = np.linspace(0,3,len(npvRaw))\n",
    "\n",
    "    y= np.array(npvRaw,dtype=float)\n",
    "\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(t,y)\n",
    "\n",
    "    area = simps(y, t)   \n",
    "    \n",
    "    return slope,r_value,area,t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": true,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "###### Area under curve and slope model (fitted data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def doModellingFitted(df,start,end):\n",
    "        \n",
    "    npvFit = df['fittedSpline'].loc[start:end]\n",
    "\n",
    "    t = np.linspace(0,3,len(npvFit))\n",
    "\n",
    "    y= np.array(npvFit,dtype=float)\n",
    "\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(t,y)\n",
    "    \n",
    "    ySmallIntegral = y-y[-1]\n",
    "    \n",
    "    y = y-100\n",
    "    \n",
    "    #pdb.set_trace()\n",
    "\n",
    "    area = simps(y, t)   \n",
    "    \n",
    "    areaSmall = simps(ySmallIntegral,t)\n",
    "    \n",
    "    return slope,r_value,area,areaSmall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": true,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "##### Rescale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": true,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "Outputs from modelling are rescaled from 1 to 100 for the purposes of image output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def doRescale(slopes,slopesF,auc,aucF,aucS):\n",
    "    \n",
    "    slopeRescaled = remap(slopes, min(slopes), max(slopes), 1, 100)\n",
    "\n",
    "    aucRescaled = remap(auc, min(auc), max(auc), 1, 100)\n",
    "        \n",
    "    slopeRescaledFit = remap(slopesF, min(slopesF), max(slopesF), 1, 100)\n",
    "\n",
    "    aucRescaledFit = remap(aucF, min(aucF), max(aucF), 1, 100)   \n",
    "    \n",
    "    aucSmallRescaledFit = remap(aucS, min(aucS), max(aucS), 1, 100)\n",
    "    \n",
    "    \n",
    "    return slopeRescaled,aucRescaled,slopeRescaledFit,aucRescaledFit,aucSmallRescaledFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def remap(x, oMin, oMax, nMin, nMax):\n",
    "\n",
    "    #check reversed input range\n",
    "    \n",
    "    reverseInput = False\n",
    "    \n",
    "    #pdb.set_trace()\n",
    "    \n",
    "    oldMin = min( oMin, oMax )\n",
    "    \n",
    "    oldMax = max( oMin, oMax )\n",
    "    \n",
    "    if not oldMin == oMin:\n",
    "    \n",
    "        reverseInput = True\n",
    "\n",
    "    #check reversed output range\n",
    "    \n",
    "    reverseOutput = False   \n",
    "    \n",
    "    newMin = min( nMin, nMax )\n",
    "    \n",
    "    newMax = max( nMin, nMax )\n",
    "    \n",
    "    if not newMin == nMin :\n",
    "    \n",
    "        reverseOutput = True\n",
    "\n",
    "    portion = (x-oldMin)*(newMax-newMin)/(oldMax-oldMin)\n",
    "    \n",
    "    if reverseInput:\n",
    "        \n",
    "        portion = (oldMax-x)*(newMax-newMin)/(oldMax-oldMin)\n",
    "\n",
    "    result = portion + newMin\n",
    "    \n",
    "    if reverseOutput:\n",
    "        \n",
    "        result = newMax - portion\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": true,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "##### Main utilisation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def doSlicing(df,nBands):\n",
    "         \n",
    "    #Get the peak locations\n",
    "\n",
    "    mask = df['peaks']== True\n",
    "\n",
    "    peakVals = df[mask]\n",
    "\n",
    "    #get the trough locations\n",
    "\n",
    "    mask = df['troughs']== True\n",
    "\n",
    "    troughVals = df[mask]\n",
    "    \n",
    "    #pdb.set_trace()\n",
    "    \n",
    "    sDate,eDate,slopes,auc,rSqr,slopesF,aucF,aucS,rSqrF,imageCount,npv,fitted = doChecking(peakVals,troughVals,df,nBands)\n",
    "    \n",
    "    modellingDf = pandas.DataFrame()\n",
    "   \n",
    "    modellingDf['sDate']= sDate\n",
    "\n",
    "    modellingDf['eDate']= eDate\n",
    "\n",
    "    modellingDf['rSqr']= rSqr\n",
    "    \n",
    "    modellingDf['rSqrF']= rSqrF\n",
    "\n",
    "    modellingDf['imageCount']= imageCount\n",
    "    \n",
    "    modellingDf['npv']= npv\n",
    "    \n",
    "    modellingDf['fitted']= fitted\n",
    "    \n",
    "    \n",
    "\n",
    "    if sum(imageCount)>0 and sum(slopes)is not 0 :\n",
    "\n",
    "        slopeRescaled,aucRescaled,slopeRescaledFit,aucRescaledFit,aucRescaledFitSmall = doRescale(slopes,slopesF,auc,aucF,aucS)\n",
    "\n",
    "        #with rescaling\n",
    "        \n",
    "        modellingDf['slope']= slopeRescaled\n",
    "\n",
    "        modellingDf['auc']= aucRescaled\n",
    "\n",
    "        modellingDf['slopeF'] = slopeRescaledFit\n",
    "\n",
    "        modellingDf['aucF'] = aucRescaledFit\n",
    "        \n",
    "        modellingDf['aucS'] = aucRescaledFitSmall\n",
    "        \n",
    "        #without rescaling\n",
    "        \n",
    "        modellingDf['slopeUsc']= slopes\n",
    "\n",
    "        modellingDf['aucUsc']= auc\n",
    "\n",
    "        modellingDf['slopeFUsc'] = slopesF\n",
    "\n",
    "        modellingDf['aucFUSc'] = aucF\n",
    "        \n",
    "        modellingDf['aucSUSc'] = aucS\n",
    "        \n",
    "           \n",
    "\n",
    "    else:\n",
    "\n",
    "        #add zeros if no data\n",
    "        \n",
    "        modellingDf['slope']= 0\n",
    "\n",
    "        modellingDf['auc'] = 0\n",
    "\n",
    "        modellingDf['slopeF'] = 0\n",
    "\n",
    "        modellingDf['aucF']= 0\n",
    "        \n",
    "        modellingDf['aucS'] = 0\n",
    "        \n",
    "        \n",
    "        modellingDf['slopeUsc']= 0\n",
    "\n",
    "        modellingDf['aucUsc'] = 0\n",
    "\n",
    "        modellingDf['slopeFUsc'] = 0\n",
    "\n",
    "        modellingDf['aucFUSc']= 0\n",
    "        \n",
    "        modellingDf['aucSUsc'] = 0\n",
    "\n",
    "    return modellingDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#simple function to find the closest date in a list of dates, i.e. find the closest image date to a monthly rainfall date\n",
    "def nearestDate(base, dates):\n",
    "    \n",
    "    #pdb.set_trace()\n",
    "        \n",
    "    nearness = { abs(base - date) : date for date in dates }\n",
    "    \n",
    "    return nearness[min(nearness.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def doPixelLoop(npv,dates,profile,f):\n",
    "    \n",
    "    display(f)#progress bar\n",
    "    \n",
    "    (nBands,nRows,nCols) = npv.shape\n",
    "    \n",
    "    noOutputBands = 300 #this is an arbitory number for output arrays\n",
    "    \n",
    "    #output arrays\n",
    "    outputSlope = np.zeros((noOutputBands,nRows,nCols))\n",
    "    \n",
    "    outputAuc = np.zeros((noOutputBands,nRows,nCols))\n",
    "    \n",
    "    outputSlopeF = np.zeros((noOutputBands,nRows,nCols))\n",
    "    \n",
    "    outputAucF = np.zeros((noOutputBands,nRows,nCols))\n",
    "    \n",
    "    outputDataFrame = pd.DataFrame()\n",
    "         \n",
    "    outputDataFrame2 = pd.DataFrame()    \n",
    "    \n",
    "    #looping over image pixel by pixel (slow - bottle neck)\n",
    "    for i in range(nRows): #for each row in image stack\n",
    "        \n",
    "        f.value = i      \n",
    "       \n",
    "        \n",
    "        for j in range(nCols): #for each column in image stack\n",
    "            \n",
    "                                    \n",
    "            drill = npv[:,i,j] #drill down thorugh image and return 1d array\n",
    "            \n",
    "            #pdb.set_trace()\n",
    "                                   \n",
    "            #call peak trough function - see func for details            \n",
    "            peaksVar,troughsVar,peaksMan,troughsMan,splDataV, splDataM,newDates,newData,firstDeriv = doDerivativePeakTrough(drill,dates,nBands)\n",
    "            \n",
    "            #Note: the manual parametrisation method has not been used any further, need to test this at some stage and compare to weighted method\n",
    "            \n",
    "            #call func that organises output from peak trough into a pandas df\n",
    "            dfV = doPandasDf(peaksVar,troughsVar,splDataV,newDates,newData,firstDeriv)\n",
    "            \n",
    "            #pdb.set_trace()\n",
    "            \n",
    "            dfV['x']=int(j)\n",
    "            \n",
    "            dfV['y']=int(i) \n",
    "           \n",
    "            #call func that slices the time series up into peak to trough components and stores in pandas dataframe\n",
    "            modelledData = doSlicing(dfV,nBands)\n",
    "            \n",
    "            modelledData['x']=int(j)\n",
    "            \n",
    "            modelledData['y']=int(i)            \n",
    "           \n",
    "            outputDataFrame = pd.concat([outputDataFrame,modelledData])   \n",
    "            \n",
    "            outputDataFrame2 = pd.concat([outputDataFrame2,dfV])\n",
    "            \n",
    "    outputDataFrame.to_csv('output.csv',index_label=\"index\")  \n",
    "    \n",
    "    outputDataFrame2.to_pickle('outputTimeSeriesPk.pkl')\n",
    "                \n",
    "    outputDataFrame2.to_csv('outputTimeSeries.csv',index_label=\"index\")             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#Function that reads the download image into memory\n",
    "def openTif(filename):\n",
    "    \n",
    "    '''\n",
    "    npv = []\n",
    "    profile = gdal.Open('chip.tif', gdal.GA_ReadOnly)\n",
    "    for i in xrange(1, profile.RasterCount+1):\n",
    "        npv.append(profile.GetRasterBand(i).ReadAsArray())\n",
    "    \n",
    "    '''\n",
    "    with rasterio.open(filename) as src:\n",
    "        npv = src.read()\n",
    "        profile = src.profile\n",
    "        \n",
    "    return npv,profile\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def doProcess(btn):\n",
    "    \n",
    "    print 'Processing started.....'\n",
    "    \n",
    "    print 'This can take up to 10 minutes'\n",
    "    \n",
    "    '''\n",
    "    method = meth.value\n",
    "    \n",
    "    if method == 's':        \n",
    "    \n",
    "        npv,profile = openTif('data/chip.tif')\n",
    "\n",
    "        temp = pandas.read_csv('data/dates.csv',header =0,parse_dates=['0'],dayfirst=True) #reads the csv into a pandas array\n",
    "        \n",
    "    elif method == 'o':\n",
    "        \n",
    "        npv,profile = openTif('data/omp/chip.tif')\n",
    "\n",
    "        temp = pandas.read_csv('data/omp/dates.csv',header =0,parse_dates=['0'],dayfirst=True) #reads the csv into a pandas array\n",
    "    '''\n",
    "    npv,profile = openTif('chip.tif')\n",
    "\n",
    "    temp = pandas.read_csv('dates.csv',header =0,parse_dates=['0'],dayfirst=True) #reads the csv into a pandas array\n",
    "\n",
    "    dates = pandas.DataFrame()\n",
    "    \n",
    "    dates['dates']=temp['0']\n",
    "    \n",
    "    b,r,c = np.shape(npv)\n",
    "    \n",
    "    f = FloatProgress(min=0, max=r)\n",
    "  \n",
    "    tic = time.clock()\n",
    "    \n",
    "    doPixelLoop(npv,dates,profile,f)\n",
    "    \n",
    "    toc = time.clock()\n",
    "    \n",
    "    elapsed = toc - tic\n",
    "    \n",
    "    print '...complete.' + \"elapsed time = \" + str(elapsed)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": true,
        "row": 4,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#meth = widgets.Dropdown(options ={'Star Transects':'s','Utilisation sites':'o'}) #\n",
    "\n",
    "#display(meth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 3,
        "hidden": false,
        "row": 4,
        "width": 3
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8aa3a37d6441119fac12e9ecba61eb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing started.....\n",
      "This can take up to 10 minutes\n"
     ]
    },
    {
     "ename": "RasterioIOError",
     "evalue": "chip.tif: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRasterioIOError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-175a3c431833>\u001b[0m in \u001b[0;36mdoProcess\u001b[1;34m(btn)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/omp/dates.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'0'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdayfirst\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#reads the csv into a pandas array\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     '''\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mnpv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprofile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopenTif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'chip.tif'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dates.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'0'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdayfirst\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#reads the csv into a pandas array\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-ae336c81f2ba>\u001b[0m in \u001b[0;36mopenTif\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     '''\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mrasterio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mnpv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mprofile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/envs/python2/lib/python2.7/site-packages/rasterio/__init__.pyc\u001b[0m in \u001b[0;36mopen\u001b[1;34m(path, mode, driver, width, height, count, crs, transform, dtype, nodata, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m             raise ValueError(\n\u001b[0;32m    234\u001b[0m                 \"mode string must be one of 'r', 'r+', or 'w', not %s\" % mode)\n\u001b[1;32m--> 235\u001b[1;33m         \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mrasterio/_base.pyx\u001b[0m in \u001b[0;36mrasterio._base.DatasetBase.start (rasterio/_base.c:4407)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mRasterioIOError\u001b[0m: chip.tif: No such file or directory"
     ]
    }
   ],
   "source": [
    "btn = widgets.Button(description=\"Process Image!\")\n",
    "\n",
    "btn.on_click(doProcess)\n",
    "\n",
    "display(btn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 2,
        "hidden": false,
        "row": 2,
        "width": 4
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "source": [
    "#### Click below to process image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 2,
        "hidden": true,
        "row": 2,
        "width": 2
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "#### Choose field site collection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#doProcess('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
